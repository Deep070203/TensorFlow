{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f2bf48-26a5-4572-bd7c-473697f1085a",
   "metadata": {},
   "source": [
    "# Self Attention for Transformer Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a85ec6-09a5-4a26-9db1-00f8dbed7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "L, d_k, d_v = 4, 8, 8 # L is length of input seq\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205e79a0-4d5c-4a40-9332-3ccd50ba50fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.03150329  1.35143838  1.31164657  0.70608428 -1.99686372  1.57223659\n",
      "   0.23142044 -0.78647482]\n",
      " [-0.65195725  0.00220972  2.0445276   1.84680798 -0.58622211 -0.30384119\n",
      "   0.83235954  1.41259665]\n",
      " [-0.57780887 -0.19889084  0.55294506  0.18665258  1.73284592  1.5020829\n",
      "   0.2913727  -0.65308217]\n",
      " [ 0.16997107  0.11478303 -0.5818222  -0.5968622   0.61287048  0.98165716\n",
      "  -0.24918441  1.27734386]]\n",
      "K\n",
      " [[-0.24171735 -0.26598006 -0.04842253 -1.22726851 -0.63681599  0.10354473\n",
      "   1.94568937 -0.11057033]\n",
      " [-1.4858841   0.72175437  0.54311908 -1.75825835  0.75474015  0.12813026\n",
      "   0.33796925 -0.52707357]\n",
      " [-1.09577162 -0.20531537 -1.68999833 -1.21844415 -1.07981043 -0.48485809\n",
      "   2.40551271 -0.21745657]\n",
      " [-1.97456094  1.30160785  1.19910193 -0.23477499 -1.49570263  0.46499355\n",
      "  -0.4010039  -0.31465564]]\n",
      "V\n",
      " [[-0.92598431  2.1010246   0.41446724 -0.93862317 -0.76331392 -0.41016499\n",
      "  -1.88031595 -0.6489421 ]\n",
      " [ 0.2738382   1.14945643 -2.24596163  0.62672257 -0.69421247  0.71430608\n",
      "   0.04153021 -0.96439499]\n",
      " [-1.32784019 -0.39618772  0.79932404  0.15384312  0.71460485  0.39551092\n",
      "  -0.58221566 -0.78671425]\n",
      " [-1.3142574  -0.54688957  2.46639004 -0.58887559  0.22254625  1.23671045\n",
      "  -0.66256083  0.78310387]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47663348-6f8a-48be-a79a-d68c378d946c",
   "metadata": {},
   "source": [
    "### Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18c2e6-c50b-4541-bd74-2e66a38c9155",
   "metadata": {},
   "source": [
    "$$ \\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg) $$\n",
    "\n",
    "$$ \\text{new V} = \\text{self attention}.V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "670f26de-453a-4f5a-b736-c6c6723fc9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67452581, -0.41342103, -1.26736471,  6.97632708],\n",
       "       [-0.40335257, -2.11102285, -2.51613643,  3.26548703],\n",
       "       [-0.37211935,  2.63014624, -2.24444445, -0.30345187],\n",
       "       [-0.22564354,  0.39459415, -0.5142258 , -1.50595883]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T)\n",
    "\n",
    "# Here, it focuses on the affinity of the word towards another words. 1st line is for 1st word and each columns represents affinity towards another word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a7859b5-02ef-4a78-bab5-61e39229bdd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8868725621213633, 1.0660734565357852, 5.448324327093019)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why we need sqrt(d_k) in denominator (to stabilize the product)\n",
    "q.var(), k.var(), np.matmul(q, k.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22f02054-71d7-4109-9b82-5f004802dac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8868725621213633, 1.0660734565357852, 0.6810405408866272)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "q.var(), k.var(), scaled.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d26e7a-28de-4ba7-b8e5-1dd964d48401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23848089, -0.14616641, -0.44808109,  2.46650409],\n",
       "       [-0.14260667, -0.74635929, -0.88958857,  1.15452401],\n",
       "       [-0.13156406,  0.92989712, -0.79353095, -0.10728644],\n",
       "       [-0.07977704,  0.1395101 , -0.18180627, -0.53243685]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bab337a-3cd0-4a65-8f67-2424ef2ce88e",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2556eba-da4c-42a4-a3b9-99072016eeb2",
   "metadata": {},
   "source": [
    "This is to ensure words don't get context from words generated in the future\n",
    "\n",
    "Not required in encoders, but required in the decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ff3cb8-b95a-43f4-a574-2c077de881f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a triangular matrix and will simulate the fact that the first word will \n",
    "# only look at itself, second will look at 1 and 2 and so on..\n",
    "mask = np.tril(np.ones( (L, L) ))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86cd0026-88e4-4fd9-89be-ef29c1855841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming 0 to inf and 1 to 0. 0 and inf coz of the softmax operation\n",
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb2c243a-35a6-44e9-a640-5b6be6980829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23848089,        -inf,        -inf,        -inf],\n",
       "       [-0.14260667, -0.74635929,        -inf,        -inf],\n",
       "       [-0.13156406,  0.92989712, -0.79353095,        -inf],\n",
       "       [-0.07977704,  0.1395101 , -0.18180627, -0.53243685]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masking to get no context from future words\n",
    "scaled + mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbda62f-8595-4294-bd6a-b5dafd83a07c",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a7be9-5943-4c0b-a727-f5d36345fd0c",
   "metadata": {},
   "source": [
    "$$ \\text{softmax} = \\frac{e^{x_i}}{\\sum_j e^x_j} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73e4a582-4cf3-4885-bc7c-28d27f14140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a vector into a probability distribution\n",
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "981fee61-a8c6-4b1f-b0de-964befd4428a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.64651438, 0.35348562, 0.        , 0.        ],\n",
       "       [0.22694122, 0.65599438, 0.11706439, 0.        ],\n",
       "       [0.26426179, 0.32905581, 0.23862923, 0.16805317]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = softmax(scaled + mask)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e4116e5-9a08-4174-a285-0dc25ac9730c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.92598431,  2.1010246 ,  0.41446724, -0.93862317, -0.76331392,\n",
       "        -0.41016499, -1.88031595, -0.6489421 ],\n",
       "       [-0.5018643 ,  1.76465893, -0.52595612, -0.38529596, -0.73888755,\n",
       "        -0.01268063, -1.20097096, -0.76045016],\n",
       "       [-0.1859505 ,  1.18446658, -1.28570613,  0.21612375, -0.54497209,\n",
       "         0.42179768, -0.46763433, -0.87200564],\n",
       "       [-0.69232082,  0.74700734, -0.02429212, -0.10406648, -0.22222414,\n",
       "         0.42886921, -0.73350902, -0.5449603 ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better encapsulate context of a word\n",
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "997a1dd1-5e71-46b0-b8e6-e664c41acabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.92598431,  2.1010246 ,  0.41446724, -0.93862317, -0.76331392,\n",
       "        -0.41016499, -1.88031595, -0.6489421 ],\n",
       "       [ 0.2738382 ,  1.14945643, -2.24596163,  0.62672257, -0.69421247,\n",
       "         0.71430608,  0.04153021, -0.96439499],\n",
       "       [-1.32784019, -0.39618772,  0.79932404,  0.15384312,  0.71460485,\n",
       "         0.39551092, -0.58221566, -0.78671425],\n",
       "       [-1.3142574 , -0.54688957,  2.46639004, -0.58887559,  0.22254625,\n",
       "         1.23671045, -0.66256083,  0.78310387]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "295435ad-fcae-4336-b2fe-9edc9bcdc0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole function\n",
    "def softmax(x):\n",
    "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "  d_k = q.shape[-1]\n",
    "  scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "  if mask is not None:\n",
    "    scaled = scaled + mask\n",
    "  attention = softmax(scaled)\n",
    "  out = np.matmul(attention, v)\n",
    "  return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f4157d3-0c96-40a6-83e8-2effff5375ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.03150329  1.35143838  1.31164657  0.70608428 -1.99686372  1.57223659\n",
      "   0.23142044 -0.78647482]\n",
      " [-0.65195725  0.00220972  2.0445276   1.84680798 -0.58622211 -0.30384119\n",
      "   0.83235954  1.41259665]\n",
      " [-0.57780887 -0.19889084  0.55294506  0.18665258  1.73284592  1.5020829\n",
      "   0.2913727  -0.65308217]\n",
      " [ 0.16997107  0.11478303 -0.5818222  -0.5968622   0.61287048  0.98165716\n",
      "  -0.24918441  1.27734386]]\n",
      "K\n",
      " [[-0.24171735 -0.26598006 -0.04842253 -1.22726851 -0.63681599  0.10354473\n",
      "   1.94568937 -0.11057033]\n",
      " [-1.4858841   0.72175437  0.54311908 -1.75825835  0.75474015  0.12813026\n",
      "   0.33796925 -0.52707357]\n",
      " [-1.09577162 -0.20531537 -1.68999833 -1.21844415 -1.07981043 -0.48485809\n",
      "   2.40551271 -0.21745657]\n",
      " [-1.97456094  1.30160785  1.19910193 -0.23477499 -1.49570263  0.46499355\n",
      "  -0.4010039  -0.31465564]]\n",
      "V\n",
      " [[-0.92598431  2.1010246   0.41446724 -0.93862317 -0.76331392 -0.41016499\n",
      "  -1.88031595 -0.6489421 ]\n",
      " [ 0.2738382   1.14945643 -2.24596163  0.62672257 -0.69421247  0.71430608\n",
      "   0.04153021 -0.96439499]\n",
      " [-1.32784019 -0.39618772  0.79932404  0.15384312  0.71460485  0.39551092\n",
      "  -0.58221566 -0.78671425]\n",
      " [-1.3142574  -0.54688957  2.46639004 -0.58887559  0.22254625  1.23671045\n",
      "  -0.66256083  0.78310387]]\n",
      "New V\n",
      " [[-0.92598431  2.1010246   0.41446724 -0.93862317 -0.76331392 -0.41016499\n",
      "  -1.88031595 -0.6489421 ]\n",
      " [-0.5018643   1.76465893 -0.52595612 -0.38529596 -0.73888755 -0.01268063\n",
      "  -1.20097096 -0.76045016]\n",
      " [-0.1859505   1.18446658 -1.28570613  0.21612375 -0.54497209  0.42179768\n",
      "  -0.46763433 -0.87200564]\n",
      " [-0.69232082  0.74700734 -0.02429212 -0.10406648 -0.22222414  0.42886921\n",
      "  -0.73350902 -0.5449603 ]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.64651438 0.35348562 0.         0.        ]\n",
      " [0.22694122 0.65599438 0.11706439 0.        ]\n",
      " [0.26426179 0.32905581 0.23862923 0.16805317]]\n"
     ]
    }
   ],
   "source": [
    "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
