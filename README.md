# Transformers from Scratch

Creating the whole Transformer mechanism with Encoder and Decoder. Based on the Attention is All You Need paper by Vaswani et al.

The image of the architure is by Ajay Halthor.

Created the Multi-Headed Attention Algorithm, Positional Encoding, Layer Normalization, PositionwiseFeedForward, MultiCrossHeadAttention and Masking using PyTorch.
