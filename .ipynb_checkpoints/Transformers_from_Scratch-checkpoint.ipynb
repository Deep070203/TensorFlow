{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6979719b-9a66-4e13-bf9d-1ef1b3040694",
   "metadata": {},
   "source": [
    "# Transformers from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5eba0b-246b-43bf-81d9-71737fe08ab0",
   "metadata": {},
   "source": [
    "Transformer [Attention Mechanism]: Encoder and Decoder. Main function is for sequence to sequence modeling [sequence is an ordered set of tokens]\n",
    "\n",
    "Encoder [Goal of Attention]: Generate vectors of high quality\n",
    "\n",
    "    Step 1: Positional Encoding of the inputs (one hot encoding of the words). We will get vectors of size 512 dimensions\n",
    "    Step 2: Passed into Encoder: It is going to generate another set of vectors. Through the attention mechanism (Multi-Head Attention and Add and Norm, Layer Normalisation), vectors are going to be much more context aware and hence higher quality.\n",
    "\n",
    "Attention in Transformers:\n",
    "    Scaled Dot-Product Attention and Multi-Head Attention\n",
    "    \n",
    "    Each word input to the transformer will have three vectors:\n",
    "        \n",
    "        Query Vector: What am I looking for [sequence length x d(k)]\n",
    "        \n",
    "        Key Vector: What can I offer\n",
    "        \n",
    "        Value vector: What I actually offer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f17ef-0a75-4c32-b0ea-c48c50e628ec",
   "metadata": {},
   "source": [
    "## Self Attention for Transformer Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7528bc39-3f6b-46d9-ac36-9860bb3b995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "L, d_k, d_v = 4, 8, 8 # L is length of input seq\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c49f94d-7683-4a1b-b587-ffb952042fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.75093004  0.64980274 -0.2989189   0.49720336 -0.27487148  1.29621948\n",
      "  -1.65384086  1.55125904]\n",
      " [ 0.03812909  0.63177235  1.00732197  1.19658816 -0.7794206  -0.77564854\n",
      "   0.05643031  0.92131849]\n",
      " [-1.92527501 -0.23311965  0.29951544  0.15592858  0.55276723  1.73631211\n",
      "  -1.33689497 -0.08537061]\n",
      " [-1.65465802 -0.89181061  0.66088938  0.57796869 -1.20558687  1.23999494\n",
      "   0.70429926 -0.65194609]]\n",
      "K\n",
      " [[ 1.16960404 -0.42379052 -1.20586281 -0.03875967 -0.8639509   0.67110154\n",
      "  -0.81708313 -0.7727469 ]\n",
      " [-0.6672614  -1.40453763  0.86421333  0.5262034   0.33517601  0.04663314\n",
      "   0.02464463 -2.64405224]\n",
      " [ 1.53010645 -0.20130136 -0.36442526 -0.94770388  0.18970751  1.69489885\n",
      "   2.54604028 -0.26950432]\n",
      " [-1.13184107 -0.19564888 -0.4287255  -1.09966455 -1.33573119  0.74126439\n",
      "   0.62854576  1.46574372]]\n",
      "V\n",
      " [[ 1.26293968  0.18046016 -1.50498972 -0.45275779 -0.49254778  0.09240977\n",
      "   1.51714337 -0.04764808]\n",
      " [-0.89338894 -1.30319967 -0.58584499 -1.12409053  1.07243493 -1.49191288\n",
      "   1.23974833 -0.72539971]\n",
      " [ 2.02894226 -0.0097814   0.08352905  0.1063474   0.35177513 -0.00477472\n",
      "  -1.18937193  0.28341608]\n",
      " [ 2.05276101  0.50959119 -0.30388456  0.62640114 -2.54713564 -1.05011528\n",
      "   0.54625447 -0.21214757]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc445b9-50f5-4f30-9056-5ae1abece109",
   "metadata": {},
   "source": [
    "### Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d372e-1e91-4398-9ee3-13be649ac535",
   "metadata": {},
   "source": [
    "$$ \\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg) $$\n",
    "\n",
    "$$ \\text{new V} = \\text{self attention}.V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "532fa5be-21ef-4165-9911-c227b44b3e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.20405952, -5.58449039, -1.82807199,  1.1665599 ],\n",
       "       [-2.08942672, -2.14463781, -3.13707311, -0.06245023],\n",
       "       [-0.67423161,  2.41200346, -3.48891673,  1.5081227 ],\n",
       "       [-0.57464896,  4.62682727,  0.70096552,  3.14498234]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T)\n",
    "\n",
    "# Here, it focuses on the affinity of the word towards another words. 1st line is for 1st word and each columns represents affinity towards another word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d938d43-7b5e-4f65-b5b2-db15e99eaf1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9405251283393438, 1.1344256995217092, 6.967783515763168)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why we need sqrt(d_k) in denominator (to stabilize the product)\n",
    "q.var(), k.var(), np.matmul(q, k.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0bb753e-a520-4d4d-bd54-2c4e7677b114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9405251283393438, 1.1344256995217092, 0.8709729394703959)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "q.var(), k.var(), scaled.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "829b6d19-16a1-493d-b325-90b450e36cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.77925272, -1.97441551, -0.64632105,  0.41244121],\n",
       "       [-0.7387239 , -0.75824397, -1.10912283, -0.02207949],\n",
       "       [-0.23837687,  0.852772  , -1.23351834,  0.53320189],\n",
       "       [-0.20316909,  1.63583047,  0.24782874,  1.11191917]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5abc42-4f04-42e7-a469-893469c5768f",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2de13e-8386-4932-90d8-bc583062491c",
   "metadata": {},
   "source": [
    "This is to ensure words don't get context from words generated in the future\n",
    "\n",
    "Not required in encoders, but required in the decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b836f28-5eae-4e29-a312-c1601f6fa6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a triangular matrix and will simulate the fact that the first word will \n",
    "# only look at itself, second will look at 1 and 2 and so on..\n",
    "mask = np.tril(np.ones( (L, L) ))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78ca8c14-8b5b-431d-a14b-6c6f56b13dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming 0 to inf and 1 to 0. 0 and inf coz of the softmax operation\n",
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e64d5302-4f91-4cdd-a302-28655e331954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.77925272,        -inf,        -inf,        -inf],\n",
       "       [-0.7387239 , -0.75824397,        -inf,        -inf],\n",
       "       [-0.23837687,  0.852772  , -1.23351834,        -inf],\n",
       "       [-0.20316909,  1.63583047,  0.24782874,  1.11191917]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masking to get no context from future words\n",
    "scaled + mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914379a1-3207-46b0-b8bd-dd54a0f83caf",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac02783-0c0a-4c9c-89d5-7388722fb8f2",
   "metadata": {},
   "source": [
    "$$ \\text{softmax} = \\frac{e^{x_i}}{\\sum_j e^x_j} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43164f7e-e112-4202-8947-1092aeef8e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a vector into a probability distribution\n",
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10537106-13c0-4352-95c1-c43665b3056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(scaled + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82f78f93-4d14-45fc-ad87-04096c8835d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.50487986, 0.49512014, 0.        , 0.        ],\n",
       "       [0.23002443, 0.68494217, 0.0850334 , 0.        ],\n",
       "       [0.07945841, 0.49981266, 0.12474   , 0.29598893]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abef56fd-af3a-4ab9-bd9b-23946c76c00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.26293968,  0.18046016, -1.50498972, -0.45275779, -0.49254778,\n",
       "         0.09240977,  1.51714337, -0.04764808],\n",
       "       [ 0.19529796, -0.5541297 , -1.04990266, -0.78514815,  0.28230668,\n",
       "        -0.69202028,  1.3797995 , -0.38321656],\n",
       "       [-0.14888492, -0.85193791, -0.74035159, -0.86503928,  0.65117052,\n",
       "        -1.00102355,  1.09699962, -0.48371724],\n",
       "       [ 0.51450888, -0.48740339, -0.49192389, -0.39913651, -0.21316402,\n",
       "        -1.0497523 ,  0.75351473, -0.39379   ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better encapsulate context of a word\n",
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6ba3d3a-6021-4200-875e-ac74656aee0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.26293968,  0.18046016, -1.50498972, -0.45275779, -0.49254778,\n",
       "         0.09240977,  1.51714337, -0.04764808],\n",
       "       [-0.89338894, -1.30319967, -0.58584499, -1.12409053,  1.07243493,\n",
       "        -1.49191288,  1.23974833, -0.72539971],\n",
       "       [ 2.02894226, -0.0097814 ,  0.08352905,  0.1063474 ,  0.35177513,\n",
       "        -0.00477472, -1.18937193,  0.28341608],\n",
       "       [ 2.05276101,  0.50959119, -0.30388456,  0.62640114, -2.54713564,\n",
       "        -1.05011528,  0.54625447, -0.21214757]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac555021-ec73-47e0-961b-de4394af9689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole function\n",
    "def softmax(x):\n",
    "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "  d_k = q.shape[-1]\n",
    "  scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "  if mask is not None:\n",
    "    scaled = scaled + mask\n",
    "  attention = softmax(scaled)\n",
    "  out = np.matmul(attention, v)\n",
    "  return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80f58b15-eeb5-4e63-89ba-70582c1ef91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.75093004  0.64980274 -0.2989189   0.49720336 -0.27487148  1.29621948\n",
      "  -1.65384086  1.55125904]\n",
      " [ 0.03812909  0.63177235  1.00732197  1.19658816 -0.7794206  -0.77564854\n",
      "   0.05643031  0.92131849]\n",
      " [-1.92527501 -0.23311965  0.29951544  0.15592858  0.55276723  1.73631211\n",
      "  -1.33689497 -0.08537061]\n",
      " [-1.65465802 -0.89181061  0.66088938  0.57796869 -1.20558687  1.23999494\n",
      "   0.70429926 -0.65194609]]\n",
      "K\n",
      " [[ 1.16960404 -0.42379052 -1.20586281 -0.03875967 -0.8639509   0.67110154\n",
      "  -0.81708313 -0.7727469 ]\n",
      " [-0.6672614  -1.40453763  0.86421333  0.5262034   0.33517601  0.04663314\n",
      "   0.02464463 -2.64405224]\n",
      " [ 1.53010645 -0.20130136 -0.36442526 -0.94770388  0.18970751  1.69489885\n",
      "   2.54604028 -0.26950432]\n",
      " [-1.13184107 -0.19564888 -0.4287255  -1.09966455 -1.33573119  0.74126439\n",
      "   0.62854576  1.46574372]]\n",
      "V\n",
      " [[ 1.26293968  0.18046016 -1.50498972 -0.45275779 -0.49254778  0.09240977\n",
      "   1.51714337 -0.04764808]\n",
      " [-0.89338894 -1.30319967 -0.58584499 -1.12409053  1.07243493 -1.49191288\n",
      "   1.23974833 -0.72539971]\n",
      " [ 2.02894226 -0.0097814   0.08352905  0.1063474   0.35177513 -0.00477472\n",
      "  -1.18937193  0.28341608]\n",
      " [ 2.05276101  0.50959119 -0.30388456  0.62640114 -2.54713564 -1.05011528\n",
      "   0.54625447 -0.21214757]]\n",
      "New V\n",
      " [[ 1.26293968  0.18046016 -1.50498972 -0.45275779 -0.49254778  0.09240977\n",
      "   1.51714337 -0.04764808]\n",
      " [ 0.19529796 -0.5541297  -1.04990266 -0.78514815  0.28230668 -0.69202028\n",
      "   1.3797995  -0.38321656]\n",
      " [-0.14888492 -0.85193791 -0.74035159 -0.86503928  0.65117052 -1.00102355\n",
      "   1.09699962 -0.48371724]\n",
      " [ 0.51450888 -0.48740339 -0.49192389 -0.39913651 -0.21316402 -1.0497523\n",
      "   0.75351473 -0.39379   ]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.50487986 0.49512014 0.         0.        ]\n",
      " [0.23002443 0.68494217 0.0850334  0.        ]\n",
      " [0.07945841 0.49981266 0.12474    0.29598893]]\n"
     ]
    }
   ],
   "source": [
    "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987a83fc-ce8c-4661-ab6e-00aec44b53e9",
   "metadata": {},
   "source": [
    "## Multi-Head Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d04a794-2b34-400d-a889-bdaa03728d76",
   "metadata": {},
   "source": [
    "Take a word. It is converted into a 512 dimension vector. Then, it is broken down into three vectors q, k, v (512 x 1). Each of these vectors are also broken up into separate pieces. \n",
    "\n",
    "Each piece is going to be a part to make an attention head. They are fed into an attention unit. Then, we generate an attention matrix which is a sequence by sequence length.\n",
    "\n",
    "It is a probability distribution so the rows add up to one. \n",
    "\n",
    "Each head will have its own matrix.\n",
    "\n",
    "This will then generate other output vectors that are concatenated to generate a vector with good contextual awareness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f0ba0b9-38fd-4a90-a376-27b9a0f652e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d3d5be0-68df-4376-b880-c91199fdcae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 4 # len of my inp sentence\n",
    "batch_size = 1 # help in parallel processing\n",
    "input_dim = 512 # vec dim of every word that goes into the attention unit \n",
    "d_model = 512 # the output of the attention unit\n",
    "x = torch.randn( (batch_size, sequence_length, input_dim) ) # randomly sampled input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dfc8c1d9-9fa4-4deb-9991-e5d86503cff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23036767-9d6f-4361-aa48-3dbe859f77ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_layer = nn.Linear(input_dim, 3 * d_model) # create query, key and value vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ae3e563-b656-45b3-acee-a87c745438f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv_layer(x) # Generate qkv vector\n",
    "qkv.shape # 1 batch, 4 words, each word vec is 1536 in size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c3b6d7c-f064-4b6b-a03c-5cdf650188a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'qkv distribution')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_val = torch.histc(qkv, bins=200, min=-3, max=3)\n",
    "x_val = np.arange(-1, 1, 0.01) * 3\n",
    "plt.bar(x_val, y_val, align='center', color=['forestgreen'])\n",
    "plt.title('qkv distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cafd43e-242e-4608-a0e1-3a896127d0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# considering 8 attention heads\n",
    "num_heads = 8\n",
    "head_dim = d_model // num_heads # 512/8 = 64\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20625634-e6b1-40c7-a82e-79e4aec2bbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv.permute(0, 2, 1, 3) # [batch_size, num_heads, sequence_length, 3 * head_dim]\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5bc78786-6b27-49fd-8cab-21d581bc148e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k, v = qkv.chunk(3, -1) # breakdown by last dim\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce49cf-a207-48a1-b3c6-20d341a17f02",
   "metadata": {},
   "source": [
    "### Self Attention for multiple heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a5f912a1-5498-4cc8-9a23-fb2e32dece9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = q.size()[-1]\n",
    "scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k) # tensors are 4 dim and not 2 dim matrix so use transpose and not .T along with the dimensions\n",
    "scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a8017696-acbf-4fe7-af21-b42cd435e88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t3/kt2c35md3j94j140w02n_3mh0000gn/T/ipykernel_41404/3717780648.py:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3679.)\n",
      "  k.T.shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4, 8, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "12f8fedf-2bde-414a-aa7f-ebdc0ee1e0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3455, -1.1219],\n",
       "        [ 1.2139, -0.0994],\n",
       "        [-0.1949,  0.6007]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy example\n",
    "y = torch.randn(2,3)\n",
    "torch.transpose(y, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0583884f-8dc0-49f9-a300-8ed65627ff56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3455, -1.1219],\n",
       "        [ 1.2139, -0.0994],\n",
       "        [-0.1949,  0.6007]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(y, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a22f773a-c3e4-4b47-8fbc-0d36e6eec09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Masking\n",
    "mask = torch.full(scaled.size(), float('-inf')) # fill up with -inf values\n",
    "mask = torch.triu(mask, diagonal = 1) # triangular matrix\n",
    "mask[0][1] # mask for input to a single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "854fb182-f998-4682-b88a-efa1a8f27ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2707,    -inf,    -inf,    -inf],\n",
       "        [-0.3121, -0.1627,    -inf,    -inf],\n",
       "        [ 0.0224, -0.3699, -0.8967,    -inf],\n",
       "        [-0.1992, -0.1902,  0.1302, -0.1646]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scaled + mask)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "160a292a-0c5e-4386-a0cf-2d6d9bc6cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled += mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5331561-3560-46f8-ae9f-cbb96bf9c2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42657718941306677"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "np.exp(-0.3121) / (np.exp(-0.3121) + np.exp(-0.01627))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "adcb576d-420a-45ff-bb18-787f8aee2661",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = F.softmax(scaled, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "961b14db-696b-4b7f-b3b1-1f2339e52412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2e10cf8d-d64c-4092-86c7-b271693042c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4627, 0.5373, 0.0000, 0.0000],\n",
       "        [0.4821, 0.3256, 0.1923, 0.0000],\n",
       "        [0.2255, 0.2276, 0.3135, 0.2335]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "646e96a6-afdf-445b-9ca0-389db6f8b6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.matmul(attention, v)\n",
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "744c3df0-29f6-45dc-8127-e0bb33196809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\n",
    "import math\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9367d14e-f6fc-42bc-a68d-67b716a33ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, attention = scaled_dot_product(q, k, v, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "125f05e4-121c-451a-8b8c-7fe008761798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4627, 0.5373, 0.0000, 0.0000],\n",
       "        [0.4821, 0.3256, 0.1923, 0.0000],\n",
       "        [0.2255, 0.2276, 0.3135, 0.2335]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention.shape\n",
    "attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0706c0de-abb9-4a1a-853f-7b4918744bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "da5f2a71-49ae-4920-821b-aa6df7163197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = values.reshape(batch_size, sequence_length, num_heads * head_dim)\n",
    "values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9ea17841-0f7b-46fb-8484-e40a1f748c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward Linear layer 512 x 512\n",
    "linear_layer = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "30851069-8a12-46cb-9454-570378de62d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = linear_layer(values)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70a7e87-42f3-471f-8056-2b79a068336e",
   "metadata": {},
   "source": [
    "Multi-Headed Attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "94943d30-1807-49c6-9522-37ad9bcc2b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "# take in a constructor to initialize parameters\n",
    "# Also a forward path\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim , 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        print(f\"values.size(): {values.size()}\")\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.size(): {out.size()}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3f01f9-f746-44b6-8224-9d50a28dc492",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee49ef-62aa-4f9c-ba24-2a73a630661e",
   "metadata": {},
   "source": [
    "We have a max length of sentence and that will be the number of vectors (vectors with no words will have dummy words in it) and each word is one-hot encoded and the vctor size is the vocab size i.e. number of words in our dictionary.\n",
    "\n",
    "We pass this into a feed forward layer where each of these vectors is going to be mapped to a 512 dim vector and the parameters here are learnable via backpropogation and the number of parameters would be the vocab size x 512 to learn.\n",
    "\n",
    "The output of this would just be a set of 512 dimensional vectors, one for each word and to this we will add positional encoding of same size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2823361a-5351-4faa-a46d-61d1cae2b86f",
   "metadata": {},
   "source": [
    "#### Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a4cd5c-0231-4999-96e7-0065854070c6",
   "metadata": {},
   "source": [
    "$$ PE(\\text{position}, 2i) = \\sin\\bigg( \\frac{ \\text{position} }{10000^\\frac{2i}{d_{model}}} \\bigg) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc3803-4479-4673-b395-ddf4527fdff8",
   "metadata": {},
   "source": [
    "$$ PE(\\text{position}, 2i+1) = \\cos\\bigg( \\frac{ \\text{position} }{10000^\\frac{2i}{d_{model}}} \\bigg) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784da178-5e2f-443b-b418-03a9ecf89f32",
   "metadata": {},
   "source": [
    "i = dimension index, d_model = Embedding length (here 512), pos = position of word in sequeunce\n",
    "\n",
    "Why? 1. Periodicity, 2. Constrained Values, 3. Easy to extrapolate for long sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f88e34a0-fe22-49db-8958-93d2060bdbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "max_seq_length = 10\n",
    "d_model = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e7c2b-9af0-4e31-ab32-79fac29f8389",
   "metadata": {},
   "source": [
    "$$ PE(\\text{position}, i) = \\sin\\bigg( \\frac{ \\text{position} }{10000^\\frac{i}{d_{model}}} \\bigg) \\text{ when i is even} $$\n",
    "\n",
    "$$ PE(\\text{position}, i) = \\cos\\bigg( \\frac{ \\text{position} }{10000^\\frac{i-1}{d_{model}}} \\bigg) \\text{ when i is odd} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "edfcf686-5614-4ab9-ae1a-db53d4851418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_i = torch.arange(0, d_model, 2).float()\n",
    "even_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ad1a11ee-b131-4ff0-8560-01f69de22095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.0000,  21.5443, 464.1590])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_denom = torch.pow(10000, even_i/d_model)\n",
    "even_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c65ae519-3a49-430f-afcd-4a9de6003ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 5.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_i = torch.arange(1, d_model, 2).float()\n",
    "odd_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3492a41-56d2-4ff4-ac50-585caf52abe5",
   "metadata": {},
   "source": [
    "even_denom and odd_denom will have same values since we do odd_i-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e254ff0e-6554-4c2f-869d-aba0d0396796",
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = even_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2b7b67e5-0aec-4df2-94d7-09691fb3f368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [9.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(max_seq_length, dtype=torch.float).reshape(max_seq_length, 1)\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7d3ba2df-758e-4564-a17b-3e25dea3e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "even_PE = torch.sin(position / denominator)\n",
    "odd_PE = torch.cos(position / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "126d2a88-761c-44b6-9d41-d4d236111d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.8415,  0.0464,  0.0022],\n",
       "        [ 0.9093,  0.0927,  0.0043],\n",
       "        [ 0.1411,  0.1388,  0.0065],\n",
       "        [-0.7568,  0.1846,  0.0086],\n",
       "        [-0.9589,  0.2300,  0.0108],\n",
       "        [-0.2794,  0.2749,  0.0129],\n",
       "        [ 0.6570,  0.3192,  0.0151],\n",
       "        [ 0.9894,  0.3629,  0.0172],\n",
       "        [ 0.4121,  0.4057,  0.0194]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "85d1c228-d90e-4600-91a3-ba992c49e78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000,  1.0000],\n",
       "        [ 0.5403,  0.9989,  1.0000],\n",
       "        [-0.4161,  0.9957,  1.0000],\n",
       "        [-0.9900,  0.9903,  1.0000],\n",
       "        [-0.6536,  0.9828,  1.0000],\n",
       "        [ 0.2837,  0.9732,  0.9999],\n",
       "        [ 0.9602,  0.9615,  0.9999],\n",
       "        [ 0.7539,  0.9477,  0.9999],\n",
       "        [-0.1455,  0.9318,  0.9999],\n",
       "        [-0.9111,  0.9140,  0.9998]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "eecd6e5d-36d4-4678-b8e6-0d35489cc3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_PE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0a28092b-8dd2-4240-9c2a-0abe44801325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 2])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the two tensors in way such that its index are even_PE, odd_PE, even_PE...\n",
    "stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b3ea96a1-5678-4ad5-a76d-b2d44a7c880d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
       "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
       "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
       "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
       "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
       "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
       "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
       "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
       "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "PE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6de06c-c0fa-43d4-abf6-b6fdcd36a06d",
   "metadata": {},
   "source": [
    "#### PE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "727559e3-6493-4988-a332-0830b13c5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "32f88b7d-86aa-4232-b9f0-f4513542a75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
       "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
       "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
       "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
       "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
       "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
       "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
       "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
       "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding(d_model=6, max_sequence_length=10)\n",
    "pe.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851d9fc-be4f-4f51-95d4-c0a964f970ef",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d434fd8-71af-4d98-b8d8-68f2485f06c0",
   "metadata": {},
   "source": [
    "Here, we focus on the ADD & NORM parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b38f5-b428-475a-93cc-60954105c638",
   "metadata": {},
   "source": [
    "Normalization:\n",
    "\n",
    "Activations of neurons will be a wide range of pos and neg values. Normalization encapsulates these values within a much smaller range and typically centered around zero and what this allows for is much more stable training as during the backprpogation phase when we actually perform a gradient step. we are taking much more even steps so it is now easier to learn and hence it is faster and also more stable training to get to the optimal position or these optimal parameter values more consistently and in a quick way.\n",
    "\n",
    "In Layer Normalization, the strategy is that we apply normalization to a neural network in this case we are going to ensure that the activation values of every neuron in every layer is normalized such that all the activation values in a layer will have like a center like zero and std of 1.\n",
    "\n",
    "Let x, y, z and o be the activation vectors for every single layer of NN. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8d8753-f7fa-4ff3-8119-7e56ad669038",
   "metadata": {},
   "source": [
    "x' = f[w_1.T x + b_1] (w_1 = weights, b_1 = bias, f = activation)\n",
    "\n",
    "y = _1[(x' - _1)/_1] + _1 (_1 = mean of activation values of y layer, _1 = std of activation values of y layer, _1, _1 = learnable params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33b127-ca3c-483b-bea9-d63d592dfec9",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "[ 0.2 0.1 0.3\n",
    "  0.5 0.1 0.1 ] -> 2 words 3 dimensions\n",
    "\n",
    "_11 = 1/3 [0.2+0.1+0.3] = 0.2\n",
    "_21 = 1/3[0.5+0.1+0.1] = 0.233\n",
    "\n",
    "_11 = (1/3{[0.2 - 0.2]^2 + [0.1 - 0.2]^2 + [0.3 - 0.2]^2})^(1/2)\n",
    "     = 0.08164\n",
    "_21 = (1/3{[0.5 - 0.233)^2 + [0.1 - 0.233]^2 + [0.1 - 0.233]^2})^(1/2)\n",
    "     = 0.1885\n",
    "\n",
    "y = (x - )/\n",
    "y = [ 0     -1.2248 1.2248\n",
    "      1.414 -0.707  -0.707 ]\n",
    "\n",
    "out =  * y + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9335e4d1-31d7-40ce-9e3f-f18c2be53ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dfd85404-9d06-43b5-9de0-d542e18f9dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.Tensor([[[0.2, 0.1, 0.3], [0.5, 0.1, 0.1]]])\n",
    "B, S, E = inputs.size()\n",
    "inputs = inputs.reshape(S, B, E)\n",
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9d9c3e71-e388-4345-bf38-dc84d7430c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_shape = inputs.size()[-2:]\n",
    "gamma = nn.Parameter(torch.ones(parameter_shape))\n",
    "beta = nn.Parameter(torch.zeros(parameter_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e9531920-44d6-4fd0-b5cd-6690de560881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma.size(), beta.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "223b3409-9c1c-4581-84ed-fde745ae056d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -2]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computing the dimensions for which we want to compute layer normalization that is the \n",
    "# batch dimension as well as the embedding dimension\n",
    "dims = [-(i+1) for i in range(len(parameter_shape))]\n",
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "475bde06-35fc-4f53-947c-3725adbb77c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = inputs.mean(dim=dims, keepdim=True)\n",
    "mean.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "12a5f795-197a-46ea-9d0e-fc885dfe7c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0817]],\n",
       "\n",
       "        [[0.1886]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = ((inputs-mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "epsilon = 1e-5\n",
    "std = (var + epsilon).sqrt()\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0ac78fe5-8cfc-458b-9687-92760097dfb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
       "\n",
       "        [[ 1.4140, -0.7070, -0.7070]]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (inputs - mean) / std\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8ed8b876-31a1-45f8-b9da-4cdd3952b6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
       "\n",
       "        [[ 1.4140, -0.7070, -0.7070]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = gamma * y + beta\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8737005-9ed0-4c68-9e4a-6b870bd5fdcc",
   "metadata": {},
   "source": [
    "#### Layer Normalization Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ad5a53a8-c05d-46bf-a941-d84269cbb52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LayerNormalization():\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, input):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        print(f\"Mean \\n ({mean.size()}): \\n {mean}\")\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        print(f\"Standard Deviation \\n ({std.size()}): \\n {std}\")\n",
    "        y = (inputs - mean) / std\n",
    "        print(f\"y \\n ({y.size()}) = \\n {y}\")\n",
    "        out = self.gamma * y  + self.beta\n",
    "        print(f\"out \\n ({out.size()}) = \\n {out}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "51908534-75a1-43a6-a148-8898719cac32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input \n",
      " (torch.Size([5, 3, 8])) = \n",
      " tensor([[[ 1.1136e+00, -1.4782e+00, -9.1520e-01,  2.7265e-01,  4.2507e-01,\n",
      "           8.3447e-03,  1.4228e+00,  3.3838e+00],\n",
      "         [-6.7590e-01, -1.5404e-01, -4.4874e-01,  1.4388e+00, -8.5254e-01,\n",
      "           2.3761e-01, -8.8014e-01,  1.3835e+00],\n",
      "         [-3.8248e-01,  1.0160e+00,  2.3692e+00,  9.0732e-01, -4.6117e-01,\n",
      "           9.0113e-02, -9.8518e-01, -9.0005e-01]],\n",
      "\n",
      "        [[ 1.8996e+00, -1.8510e-02,  8.0088e-01, -1.1465e+00,  8.3055e-01,\n",
      "          -2.7292e-01, -2.0515e-01,  7.0747e-01],\n",
      "         [-1.0345e-01,  8.4277e-01,  2.7325e+00,  7.0761e-01, -1.4421e+00,\n",
      "          -9.5696e-01,  9.6094e-01,  3.2582e-01],\n",
      "         [ 7.3276e-01, -8.4486e-01, -2.2637e-01, -8.7309e-01, -4.3827e-01,\n",
      "           3.1612e-01,  3.4470e-01,  9.7033e-02]],\n",
      "\n",
      "        [[-5.9823e-01, -2.3441e-01, -1.5997e+00,  5.9365e-01, -3.2336e-01,\n",
      "          -4.9532e-01, -2.2306e-01, -1.1121e+00],\n",
      "         [ 5.6833e-01, -1.3622e-01,  3.2243e-01, -1.3517e+00, -2.4649e+00,\n",
      "          -2.7831e-01,  1.3344e+00, -3.8272e-01],\n",
      "         [ 6.0161e-01, -2.0641e+00,  1.8410e+00,  5.8889e-01, -8.0561e-04,\n",
      "           7.8546e-01,  1.0959e+00,  1.1753e+00]],\n",
      "\n",
      "        [[ 8.5226e-01, -9.7314e-01,  1.0678e+00, -7.9533e-01, -1.9112e+00,\n",
      "           4.9228e-01,  6.1463e-02,  1.7189e+00],\n",
      "         [-8.8825e-01, -1.2833e+00,  2.0277e+00, -3.7207e-01,  1.2456e+00,\n",
      "          -1.5024e+00,  5.7612e-01, -8.5499e-01],\n",
      "         [ 6.4681e-01,  2.0083e+00,  7.9422e-01,  8.5161e-02, -5.9004e-01,\n",
      "           7.8383e-01,  4.7499e-01,  7.3881e-01]],\n",
      "\n",
      "        [[-9.0436e-01, -4.1005e-01, -4.1024e-01,  7.7193e-01, -3.3968e-01,\n",
      "          -2.2835e-01,  4.5595e-01,  1.1142e+00],\n",
      "         [ 4.0858e-01, -9.4954e-01,  1.5839e-01,  8.6002e-01,  1.2166e+00,\n",
      "           1.2646e+00,  2.2436e-01,  4.7769e-01],\n",
      "         [-4.1359e-01, -2.3418e-02,  7.1673e-01, -1.1993e+00,  6.2640e-01,\n",
      "           6.8490e-02,  2.4455e-01, -4.2816e-01]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "sentence_length = 5\n",
    "embedding_dim = 8 \n",
    "inputs = torch.randn(sentence_length, batch_size, embedding_dim)\n",
    "\n",
    "print(f\"input \\n ({inputs.size()}) = \\n {inputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0a1d17dc-cd92-46fa-bbcc-de98c659f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = LayerNormalization(inputs.size()[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "22658b26-9d22-4cc0-86c3-4455ed256caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean \n",
      " (torch.Size([5, 3, 1])): \n",
      " tensor([[[ 0.5291],\n",
      "         [ 0.0061],\n",
      "         [ 0.2067]],\n",
      "\n",
      "        [[ 0.3244],\n",
      "         [ 0.3834],\n",
      "         [-0.1115]],\n",
      "\n",
      "        [[-0.4991],\n",
      "         [-0.2986],\n",
      "         [ 0.5029]],\n",
      "\n",
      "        [[ 0.0641],\n",
      "         [-0.1315],\n",
      "         [ 0.6178]],\n",
      "\n",
      "        [[ 0.0062],\n",
      "         [ 0.4576],\n",
      "         [-0.0510]]])\n",
      "Standard Deviation \n",
      " (torch.Size([5, 3, 1])): \n",
      " tensor([[[1.4032],\n",
      "         [0.8820],\n",
      "         [1.0767]],\n",
      "\n",
      "        [[0.8682],\n",
      "         [1.2009],\n",
      "         [0.5460]],\n",
      "\n",
      "        [[0.6091],\n",
      "         [1.0977],\n",
      "         [1.0917]],\n",
      "\n",
      "        [[1.1328],\n",
      "         [1.1947],\n",
      "         [0.6866]],\n",
      "\n",
      "        [[0.6487],\n",
      "         [0.6630],\n",
      "         [0.5856]]])\n",
      "y \n",
      " (torch.Size([5, 3, 8])) = \n",
      " tensor([[[ 4.1651e-01, -1.4305e+00, -1.0293e+00, -1.8276e-01, -7.4135e-02,\n",
      "          -3.7111e-01,  6.3686e-01,  2.0344e+00],\n",
      "         [-7.7318e-01, -1.8153e-01, -5.1564e-01,  1.6243e+00, -9.7344e-01,\n",
      "           2.6251e-01, -1.0047e+00,  1.5617e+00],\n",
      "         [-5.4721e-01,  7.5159e-01,  2.0084e+00,  6.5069e-01, -6.2030e-01,\n",
      "          -1.0829e-01, -1.1070e+00, -1.0279e+00]],\n",
      "\n",
      "        [[ 1.8144e+00, -3.9502e-01,  5.4879e-01, -1.6943e+00,  5.8296e-01,\n",
      "          -6.8806e-01, -6.1000e-01,  4.4120e-01],\n",
      "         [-4.0539e-01,  3.8253e-01,  1.9561e+00,  2.6998e-01, -1.5201e+00,\n",
      "          -1.1161e+00,  4.8092e-01, -4.7938e-02],\n",
      "         [ 1.5462e+00, -1.3431e+00, -2.1038e-01, -1.3948e+00, -5.9846e-01,\n",
      "           7.8314e-01,  8.3550e-01,  3.8191e-01]],\n",
      "\n",
      "        [[-1.6280e-01,  4.3448e-01, -1.8069e+00,  1.7939e+00,  2.8846e-01,\n",
      "           6.1466e-03,  4.5312e-01, -1.0065e+00],\n",
      "         [ 7.8976e-01,  1.4792e-01,  5.6574e-01, -9.5935e-01, -1.9735e+00,\n",
      "           1.8475e-02,  1.4876e+00, -7.6643e-02],\n",
      "         [ 9.0414e-02, -2.3514e+00,  1.2257e+00,  7.8769e-02, -4.6139e-01,\n",
      "           2.5882e-01,  5.4321e-01,  6.1589e-01]],\n",
      "\n",
      "        [[ 6.9572e-01, -9.1563e-01,  8.8597e-01, -7.5866e-01, -1.7437e+00,\n",
      "           3.7795e-01, -2.3414e-03,  1.4607e+00],\n",
      "         [-6.3344e-01, -9.6412e-01,  1.8072e+00, -2.0140e-01,  1.1526e+00,\n",
      "          -1.1475e+00,  5.9223e-01, -6.0560e-01],\n",
      "         [ 4.2300e-02,  2.0254e+00,  2.5702e-01, -7.7577e-01, -1.7592e+00,\n",
      "           2.4188e-01, -2.0795e-01,  1.7631e-01]],\n",
      "\n",
      "        [[-1.4036e+00, -6.4160e-01, -6.4189e-01,  1.1804e+00, -5.3313e-01,\n",
      "          -3.6150e-01,  6.9334e-01,  1.7080e+00],\n",
      "         [-7.3910e-02, -2.1225e+00, -4.5130e-01,  6.0703e-01,  1.1448e+00,\n",
      "           1.2173e+00, -3.5179e-01,  3.0323e-02],\n",
      "         [-6.1916e-01,  4.7162e-02,  1.3112e+00, -1.9609e+00,  1.1569e+00,\n",
      "           2.0412e-01,  5.0479e-01, -6.4404e-01]]])\n",
      "out \n",
      " (torch.Size([5, 3, 8])) = \n",
      " tensor([[[ 4.1651e-01, -1.4305e+00, -1.0293e+00, -1.8276e-01, -7.4135e-02,\n",
      "          -3.7111e-01,  6.3686e-01,  2.0344e+00],\n",
      "         [-7.7318e-01, -1.8153e-01, -5.1564e-01,  1.6243e+00, -9.7344e-01,\n",
      "           2.6251e-01, -1.0047e+00,  1.5617e+00],\n",
      "         [-5.4721e-01,  7.5159e-01,  2.0084e+00,  6.5069e-01, -6.2030e-01,\n",
      "          -1.0829e-01, -1.1070e+00, -1.0279e+00]],\n",
      "\n",
      "        [[ 1.8144e+00, -3.9502e-01,  5.4879e-01, -1.6943e+00,  5.8296e-01,\n",
      "          -6.8806e-01, -6.1000e-01,  4.4120e-01],\n",
      "         [-4.0539e-01,  3.8253e-01,  1.9561e+00,  2.6998e-01, -1.5201e+00,\n",
      "          -1.1161e+00,  4.8092e-01, -4.7938e-02],\n",
      "         [ 1.5462e+00, -1.3431e+00, -2.1038e-01, -1.3948e+00, -5.9846e-01,\n",
      "           7.8314e-01,  8.3550e-01,  3.8191e-01]],\n",
      "\n",
      "        [[-1.6280e-01,  4.3448e-01, -1.8069e+00,  1.7939e+00,  2.8846e-01,\n",
      "           6.1466e-03,  4.5312e-01, -1.0065e+00],\n",
      "         [ 7.8976e-01,  1.4792e-01,  5.6574e-01, -9.5935e-01, -1.9735e+00,\n",
      "           1.8475e-02,  1.4876e+00, -7.6643e-02],\n",
      "         [ 9.0414e-02, -2.3514e+00,  1.2257e+00,  7.8769e-02, -4.6139e-01,\n",
      "           2.5882e-01,  5.4321e-01,  6.1589e-01]],\n",
      "\n",
      "        [[ 6.9572e-01, -9.1563e-01,  8.8597e-01, -7.5866e-01, -1.7437e+00,\n",
      "           3.7795e-01, -2.3414e-03,  1.4607e+00],\n",
      "         [-6.3344e-01, -9.6412e-01,  1.8072e+00, -2.0140e-01,  1.1526e+00,\n",
      "          -1.1475e+00,  5.9223e-01, -6.0560e-01],\n",
      "         [ 4.2300e-02,  2.0254e+00,  2.5702e-01, -7.7577e-01, -1.7592e+00,\n",
      "           2.4188e-01, -2.0795e-01,  1.7631e-01]],\n",
      "\n",
      "        [[-1.4036e+00, -6.4160e-01, -6.4189e-01,  1.1804e+00, -5.3313e-01,\n",
      "          -3.6150e-01,  6.9334e-01,  1.7080e+00],\n",
      "         [-7.3910e-02, -2.1225e+00, -4.5130e-01,  6.0703e-01,  1.1448e+00,\n",
      "           1.2173e+00, -3.5179e-01,  3.0323e-02],\n",
      "         [-6.1916e-01,  4.7162e-02,  1.3112e+00, -1.9609e+00,  1.1569e+00,\n",
      "           2.0412e-01,  5.0479e-01, -6.4404e-01]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = layer_norm.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e7337d-f9e4-456e-82b5-e65833c83fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
